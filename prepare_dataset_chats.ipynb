{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from constrerl.annotator import (\n",
    "    Annotator,\n",
    "    EnumERLModel,\n",
    "    StringERLModel,\n",
    "    convert_to_enum_model,\n",
    "    convert_to_string_model,\n",
    "    load_test,\n",
    "    load_train,\n",
    "    Article,\n",
    ")\n",
    "from llama_cpp import Llama, ChatCompletionRequestMessage, LlamaGrammar\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib as pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_dir = pl.Path(\"data/annotations/train\")\n",
    "test_data_dir = pl.Path(\"data/annotations/dev\")\n",
    "train_data_files = list(train_data_dir.glob(\"*.json\"))\n",
    "test_data_files = list(test_data_dir.glob(\"*.json\"))\n",
    "\n",
    "train_data: dict[str, Article] = {}\n",
    "for file in train_data_files:\n",
    "    train_data.update(load_train(file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1567"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building train data chats train_data_chats_bronze.json: 100%|██████████| 749/749 [01:10<00:00, 10.58it/s]\n",
      "Building train data chats train_data_chats_gold.json: 100%|██████████| 208/208 [00:18<00:00, 11.08it/s]\n",
      "Building train data chats train_data_chats_platinum.json: 100%|██████████| 111/111 [00:10<00:00, 10.44it/s]\n",
      "Building train data chats train_data_chats_silver.json: 100%|██████████| 499/499 [00:51<00:00,  9.77it/s]\n",
      "Building train data chats train_data_chats_dev.json: 100%|██████████| 40/40 [00:03<00:00, 10.73it/s]\n",
      "Building train data chats train_data_chats.json: 100%|██████████| 1567/1567 [02:31<00:00, 10.36it/s]\n"
     ]
    }
   ],
   "source": [
    "annotator = Annotator()\n",
    "failed_articles: list[tuple[Article, Exception]] = []\n",
    "\n",
    "\n",
    "def export_train_date(train_data: dict[str, Article], path: pl.Path):\n",
    "    train_data_chats = []\n",
    "    for article in tqdm(\n",
    "        train_data.values(), desc=\"Building train data chats \" + path.name\n",
    "    ):\n",
    "        # try:\n",
    "        similar_articles = annotator.find_similar_examples(article.metadata)\n",
    "        full_chat = similar_articles + [article]\n",
    "        chat_elements = [Annotator.prompt_and_respone(article) for article in full_chat]\n",
    "        chat: list[ChatCompletionRequestMessage] = []\n",
    "        for i, chat_element in enumerate(chat_elements):\n",
    "            for chat_message in chat_element:\n",
    "                chat.append(chat_message)\n",
    "        train_data_chats.append(\n",
    "            {\n",
    "                \"article\": article.metadata.title,\n",
    "                \"messages\": chat,\n",
    "            }\n",
    "        )\n",
    "    # except Exception as e:\n",
    "    #     failed_articles.append(\n",
    "    #         (\n",
    "    #             article,\n",
    "    #             e,\n",
    "    #         )\n",
    "    #     )\n",
    "\n",
    "    with open(path, \"w\") as f:\n",
    "        json.dump(train_data_chats, f)\n",
    "    # jsonl dump\n",
    "    with open(path.with_suffix(\".jsonl\"), \"w\") as f:\n",
    "        for chat in train_data_chats:\n",
    "            # with system prompt\n",
    "            del chat[\"article\"]\n",
    "            chat[\"messages\"] = annotator.example_messages + chat[\"messages\"]\n",
    "            f.write(json.dumps(chat) + \"\\n\")\n",
    "\n",
    "\n",
    "for train_data_file in train_data_files + test_data_files:\n",
    "    collection_name = train_data_file.name.split(\".\")[0].split(\"_\")[-1]\n",
    "    export_train_date(\n",
    "        load_train(train_data_file),\n",
    "        pl.Path(\n",
    "            *train_data_file.parts[:-3], f\"train_data_chats_{collection_name}.json\"\n",
    "        ),\n",
    "    )\n",
    "export_train_date(train_data, pl.Path(\"data/annotations/train_data_chats.json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_relations_all = {}\n",
    "for article, err in failed_articles:\n",
    "    missing_relations: dict[str, dict[str, str]] = {}\n",
    "    for e in err.errors():\n",
    "        index = e[\"loc\"][1]\n",
    "        spo_id = e[\"loc\"][-1]\n",
    "        if index not in missing_relations:\n",
    "            missing_relations[index] = {\n",
    "                \"subject_label\": None,\n",
    "                \"predicate\": None,\n",
    "                \"object_label\": None,\n",
    "            }\n",
    "        missing_relations[index][spo_id] = e[\"input\"]\n",
    "    for index, missing_relation in missing_relations.items():\n",
    "        names = missing_relation.values()\n",
    "        spo= \"->\".join(names)\n",
    "        if spo not in missing_relations_all:\n",
    "            missing_relations_all[ \"->\".join(names)] = []\n",
    "        missing_relations_all[spo].append(article.metadata.title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "relations_missing = []\n",
    "for spo, articles in missing_relations_all.items():\n",
    "    s, p, o = spo.split(\"->\")\n",
    "    relations_missing.append(   \n",
    "            {\n",
    "                \"heads\": [s],\n",
    "                \"tails\": [o],\n",
    "                \"predicate\": [p],\n",
    "            }\n",
    "    )\n",
    "print(json.dumps(relations_missing, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"missing_relations.txt\", \"w\") as f:\n",
    "    for spo, titles in missing_relations_all.items():\n",
    "        f.write(f\"{spo}:\\n\")\n",
    "        for title in titles:\n",
    "            f.write(f\"\\t{title}\\n\")\n",
    "        f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"err.log\", \"w\") as f:\n",
    "    for article, e in failed_articles:\n",
    "        f.write(f\"{article.model_dump_json(indent=2)}\\n {e}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
