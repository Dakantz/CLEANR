[project]
name = "gutbrainie"
version = "0.1.0"
description = "Add your description here"
readme = "README.md"
requires-python = ">=3.12"
dependencies = [
    "sigstore>=3.6.1",
    "transformers>=4.48.3",
    "llama-cpp-agent",
    "llama-cpp-python",
    "tqdm>=4.67.1",
    "ipykernel",
    "betterproto==2.0.0b6",
    "json-repair>=0.36.0",
    "langchain-huggingface>=0.1.2",
    "openai>=1.62.0",
    "langchain[openai]>=0.3.18",
    "datasets>=3.2.0",
    "torch==2.5.1",
    "torchvision>=0.20.1",
    "torchao>=0.8.0",
    "torchaudio>=2.5.1",
    "torchtune>=0.5.0",
    "triton-nightly",
]

[tool.uv.sources]
llama-cpp-python = [
    { index = "llama-metal", marker = "platform_system == 'Darwin'" },
    { index = "llama-gpu", marker = "platform_system == 'Linux'" },
]
onnxruntime-gpu = [
    { index = "onnxruntime-gpu", marker = "platform_system == 'Linux'" },
]
llama-cpp-agent = [
    { git = "https://github.com/Dakantz/llama-cpp-agent.git", branch = "fix-gbnf-generation-trailing-bracket" },
]
triton-nightly = [{ index = "triton-nightly" }]

[[tool.uv.index]]
name = "llama-metal"
url = "https://abetlen.github.io/llama-cpp-python/whl/metal/"
explicit = true

[[tool.uv.index]]
name = "llama-gpu"
url = "https://abetlen.github.io/llama-cpp-python/whl/cu124/"
explicit = true

[[tool.uv.index]]
name = "triton-nightly"
url = "https://aiinfra.pkgs.visualstudio.com/PublicPackages/_packaging/Triton-Nightly/pypi/simple/"
explicit = true