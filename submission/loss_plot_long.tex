% This file was created with tikzplotlib v0.10.1.post13.
\begin{tikzpicture}

\definecolor{darkslategrey38}{RGB}{38,38,38}
\definecolor{lavender234234242}{RGB}{234,234,242}

\begin{axis}[
axis background/.style={fill=lavender234234242},
axis line style={white},
height=0.4\linewidth,
legend cell align={left},
legend style={fill opacity=0.8, draw opacity=1, text opacity=1, draw=none, fill=lavender234234242},
tick align=outside,
tick pos=left,
title={Loss for Hermes 3.2.3B LoRA with longer training},
width=0.8\linewidth,
x grid style={white},
xlabel=\textcolor{darkslategrey38}{Steps},
xmajorgrids,
xmin=-14.5, xmax=304.5,
xtick style={color=darkslategrey38},
y grid style={white},
ylabel=\textcolor{darkslategrey38}{Loss},
ymajorgrids,
ymin=0.000488027371466158, ymax=1.23763846289366,
ytick style={color=darkslategrey38}
]
\addplot [line width=0.7pt, blue]
table {%
0 1.11178469657898
1 1.12057936191559
2 1.18140435218811
3 1.15005373954773
4 1.07806575298309
5 1.11117351055145
6 1.16565954685211
7 1.05792796611786
8 1.02783191204071
9 1.01650631427765
10 1.04681217670441
11 0.982317805290222
12 0.974264860153198
13 1.03442275524139
14 0.971921145915985
15 0.932174742221832
16 0.991092264652252
17 0.839448094367981
18 0.922559380531311
19 1.00181937217712
20 0.837959289550781
21 0.902639210224152
22 0.860980451107025
23 0.859091877937317
24 0.763524413108826
25 0.875791668891907
26 0.89684784412384
27 0.92512458562851
28 0.841325402259827
29 0.861036598682404
30 0.867496848106384
31 0.799504220485687
32 0.855225682258606
33 0.820230722427368
34 0.804612398147583
35 0.813830137252808
36 0.72761857509613
37 0.732169270515442
38 0.824443101882935
39 0.751291036605835
40 0.797849893569946
41 0.758847594261169
42 0.703785300254822
43 0.733690083026886
44 0.665155172348022
45 0.761186003684998
46 0.757100403308868
47 0.824322581291199
48 0.710712313652039
49 0.672920227050781
50 0.69872909784317
51 0.694666028022766
52 0.700102686882019
53 0.648261725902557
54 0.633494198322296
55 0.648023843765259
56 0.699615180492401
57 0.6413614153862
58 0.613428056240082
59 0.646861970424652
60 0.642102837562561
61 0.585073232650757
62 0.6616131067276
63 0.742032468318939
64 0.597449719905853
65 0.568720519542694
66 0.48861101269722
67 0.511313021183014
68 0.558652281761169
69 0.444943130016327
70 0.491410315036774
71 0.555921316146851
72 0.555969059467316
73 0.437313973903656
74 0.552483141422272
75 0.469756215810776
76 0.434488296508789
77 0.450830698013306
78 0.421236753463745
79 0.46730250120163
80 0.421954154968262
81 0.448475360870361
82 0.378673404455185
83 0.456171065568924
84 0.404048770666122
85 0.325225055217743
86 0.407876431941986
87 0.351978927850723
88 0.35479399561882
89 0.37192776799202
90 0.334297448396683
91 0.286612212657928
92 0.338957458734512
93 0.309469521045685
94 0.355308890342712
95 0.335925281047821
96 0.34693244099617
97 0.297293424606323
98 0.342967629432678
99 0.222495779395103
100 0.297256231307983
101 0.328780055046082
102 0.294574111700058
103 0.297976583242416
104 0.285363763570786
105 0.283053129911423
106 0.256302177906036
107 0.241739600896835
108 0.270791828632355
109 0.242335692048073
110 0.338341593742371
111 0.220349758863449
112 0.183843106031418
113 0.276238948106766
114 0.253548681735992
115 0.243857845664024
116 0.170188382267952
117 0.250314921140671
118 0.260458260774612
119 0.307527452707291
120 0.210692495107651
121 0.204731807112694
122 0.25394082069397
123 0.274805724620819
124 0.226896062493324
125 0.188468664884567
126 0.239483892917633
127 0.19634573161602
128 0.269914418458939
129 0.227259621024132
130 0.224379628896713
131 0.16469894349575
132 0.229462698101997
133 0.200375095009804
134 0.242005079984665
135 0.169905349612236
136 0.217266008257866
137 0.159627556800842
138 0.154308214783669
139 0.202558055520058
140 0.206998571753502
141 0.167471453547478
142 0.162439718842506
143 0.144464194774628
144 0.168757021427155
145 0.172192335128784
146 0.176658749580383
147 0.154914170503616
148 0.215893492102623
149 0.186412245035172
150 0.194727212190628
151 0.237688571214676
152 0.215847849845886
153 0.161388427019119
154 0.127854436635971
155 0.14817476272583
156 0.144060298800468
157 0.160754948854446
158 0.176728948950768
159 0.125472083687782
160 0.168122783303261
161 0.165831610560417
162 0.152164489030838
163 0.115041442215443
164 0.126112520694733
165 0.133868724107742
166 0.188214406371117
167 0.147569000720978
168 0.139098420739174
169 0.134809955954552
170 0.223146453499794
171 0.123477667570114
172 0.0961095839738846
173 0.118405677378178
174 0.138410240411758
175 0.174390599131584
176 0.140251696109772
177 0.17102886736393
178 0.122376672923565
179 0.156155779957771
180 0.159063056111336
181 0.104663424193859
182 0.136211171746254
183 0.104529745876789
184 0.170179665088654
185 0.166601896286011
186 0.126831412315369
187 0.135884463787079
188 0.0819924920797348
189 0.124548122286797
190 0.106843419373035
191 0.152636885643005
192 0.164503067731857
193 0.20761302113533
194 0.138225957751274
195 0.0989241600036621
196 0.0943560004234314
197 0.13457453250885
198 0.10649224370718
199 0.114199422299862
200 0.108050018548965
201 0.0943215936422348
202 0.116944201290607
203 0.114699251949787
204 0.0968815311789513
205 0.117920622229576
206 0.117597453296185
207 0.123292870819569
208 0.138736248016357
209 0.105243384838104
210 0.126860782504082
211 0.0853931605815887
212 0.12246809899807
213 0.0931260734796524
214 0.144465148448944
215 0.0819265097379684
216 0.0847867727279663
217 0.114672638475895
218 0.111139997839928
219 0.134676098823547
220 0.132514029741287
221 0.0814253985881805
222 0.0952980071306229
223 0.11310613900423
224 0.0915154591202736
225 0.112331487238407
226 0.153326034545898
227 0.117162145674229
228 0.0619238615036011
229 0.113906733691692
230 0.156708568334579
231 0.0968992412090302
232 0.103519059717655
233 0.106207683682442
234 0.119658768177032
235 0.0833970457315445
236 0.119050353765488
237 0.108605675399303
238 0.0822521671652794
239 0.130462288856506
240 0.106827609241009
241 0.1356181204319
242 0.0808603316545486
243 0.0996185019612312
244 0.110790193080902
245 0.0972217172384262
246 0.108838573098183
247 0.128364861011505
248 0.0886054784059525
249 0.0567221380770206
250 0.0743326842784882
251 0.127112478017807
252 0.115212053060532
253 0.116995975375175
254 0.113836333155632
255 0.0997074469923973
256 0.124287135899067
257 0.0882003381848335
258 0.157710075378418
259 0.106378108263016
260 0.120869718492031
261 0.100569315254688
262 0.092155434191227
263 0.132515072822571
264 0.0642933249473572
265 0.11510244756937
266 0.0967535078525543
267 0.0819042325019836
268 0.125142723321915
269 0.110360838472843
270 0.0890418961644173
271 0.126641914248466
272 0.103108920156956
273 0.110581241548061
274 0.124498046934605
275 0.121975995600224
276 0.108560055494308
277 0.105519965291023
278 0.0829115211963654
279 0.154830425977707
280 0.102490462362766
281 0.159909665584564
282 0.0994603112339973
283 0.11594021320343
284 0.0952632948756218
285 0.116698525846004
286 0.114943109452724
287 0.0894197300076485
288 0.0960032418370247
289 0.124003805220127
290 0.0982977449893951
};
\addlegendentry{loss}
\end{axis}

\end{tikzpicture}
